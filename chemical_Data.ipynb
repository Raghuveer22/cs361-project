{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6186551,"sourceType":"datasetVersion","datasetId":3550714}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def intialise_parameters(lenw):\n    w = np.random.randn(1,lenw)\n    b = 0\n    return w,b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward_prop(X,w,b):\n    z = np.dot(w,X) + b\n    return z","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cost_function(z,y,reg_penalty='',penalty_factor=0):\n    m = y.shape[1]\n    J = (1/(2*m))*np.sum(np.square(z-y),dtype=np.float64)\n    penalty=0\n    \n    if reg_penalty=='L1':\n        penalty = penalty_factor * np.sum(np.abs(w))\n    elif reg_penalty=='L2':\n        penalty =(penalty_factor / (2 * m))  * np.sum(np.square(w))\n    \n    return J+penalty","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def back_prop(X, y, z, reg_penalty=\"\", penalty_factor=0):\n    m = y.shape[1]\n    dz = (1/m)*(z-y)\n    dw = np.dot(dz,X.T)\n    if reg_penalty==\"L1\":\n        l1_gradient = penalty_factor * np.sign(w)\n        dw += l1_gradient\n    elif reg_penalty==\"L2\":\n        l2_gradient = (penalty_factor / m) * w\n        dw += l2_gradient\n    db = np.sum(dz)\n    return dw,db\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent_update(w,b,dw,db,learning_rate=0.01):\n    w = w - learning_rate*dw\n    b = b - learning_rate*db\n    return w,b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_regresssion_model(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=50,stop_loss=0.01,metric='RMSE',reg_penalty=\"\",penalty_factor=0):\n    X_train=X_train.T\n    X_val=X_val.T\n    y_train_arr = np.array([y_train])\n    y_val_arr=np.array([y_val])\n    \n    lenw = X_train.shape[0]\n    w,b = intialise_parameters(lenw)\n    \n    costs_train = []\n    m_train = y_train_arr.shape[1]\n    m_val = y_val_arr.shape[1]\n    errors=[]\n    error=0\n    for i in range(1,epochs+1):\n        z_train = forward_prop(X_train,w,b)\n        cost_train = cost_function(z_train,y_train_arr,reg_penalty,penalty_factor)\n        dw, db = back_prop(X_train,y_train_arr,z_train,reg_penalty,penalty_factor)\n        w,b = gradient_descent_update(w,b,dw,db,learning_rate)\n        costs_train.append(cost_train)\n        if cost_train<stop_loss:\n            print('reached stoploss')\n            break\n        \n        z_val = forward_prop(X_val,w,b)    \n        \n        if metric==\"MAE\":\n            error = (1/m_val)*np.sum(np.abs(z_val - y_val_arr),dtype=np.float64)\n        elif metric==\"MSE\":\n            error = (1/m_val)*np.sum(np.square(z_val - y_val_arr),dtype=np.float64)\n        elif metric == \"RMSE\":\n            error = np.sqrt((1/m_val) * np.sum(np.square(z_val - y_val_arr),dtype=np.float64),dtype=np.float64) \n        errors.append(error)\n    return z_val,y_val_arr,w,b,errors\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/water-quality-dataset-with-wqi-result/WQI Dataset.csv',index_col='Unnamed: 0')\nprint(dataset.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Selecting the desired columns\nselected_columns = ['Alkalinity-total (as CaCO3)', 'Ammonia-Total (as N)',\n                   'BOD - 5 days (Total)', 'Chloride', 'Conductivity @25°C',\n                   'Dissolved Oxygen', 'ortho-Phosphate (as P) - unspecified', 'pH',\n                   'Temperature', 'Total Hardness (as CaCO3)', 'True Colour']\n\ndataset = dataset[selected_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_scale = ['Alkalinity-total (as CaCO3)', 'Ammonia-Total (as N)', 'Chloride', 'Conductivity @25°C',\n                   'Dissolved Oxygen', 'ortho-Phosphate (as P) - unspecified', 'pH',\n                   'Temperature', 'Total Hardness (as CaCO3)', 'True Colour']\n\n\nmeans = dataset[columns_to_scale].mean()\nstd_devs = dataset[columns_to_scale].std()\ndataset[columns_to_scale] = (dataset[columns_to_scale] - means) / std_devs\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=dataset['BOD_avg']\ny.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=dataset.drop(columns=['BOD_avg'])\nX.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split_custom(X, y, test_size=0.2, random_state=None):\n    # Set seed for reproducibility\n    if random_state is not None:\n        np.random.seed(random_state)\n    \n    # Shuffle indices\n    num_samples = len(X)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    \n    # Determine split index\n    split_index = int((1 - test_size) * num_samples)\n    \n    # Split data\n    X_train, X_test = X.iloc[indices[:split_index]], X.iloc[indices[split_index:]]\n    y_train, y_test = y.iloc[indices[:split_index]], y.iloc[indices[split_index:]]\n    \n    return X_train, X_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.2, random_state=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z_val,y_val_arr,w,b,errors_n=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=700)\nprint(w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(errors_n)\nplt.xlabel('Iterations/epochs')\nplt.ylabel('RMSE')\n\nplt.title('epochs :- 700  learning_rate 0.01' )\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# z_val,y_val,w,b,errors=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.01,epochs=50)\n# epchs 50 \n# learning rate 0.02 - 1\nfinal_error = []\nlearningRates=[]\n\nfor i in range(2, 66):\n    learningRate = (i)/100\n    z_val,y_val_arr,w,b,errors=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate = learningRate,epochs=400)\n    final_error.append(errors[-1])\n    learningRates.append(learningRate)\n\nplt.plot(learningRates,final_error)  \nplt.xlabel('learning_rate')\nplt.ylabel('RMSE')\nplt.title('Learning Rate vs RMSE')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimal learning rate would be at 0.1 to 0.2","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.plot(z_val[0], label='Predicted', color='blue')\nplt.plot(y_val_arr[0], label='Actual', color='red')\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.title('Plot of Predicted and Actual Values (Validation Data)')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_regression = linear_model.LinearRegression()\nmodel = linear_regression.fit(X_train,y_train)\nprediction = linear_regression.predict(X_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_val_with_sklearn = (1/len(y_val))*np.sum(np.abs(prediction-y_val.T))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_val_with_sklearn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30, 4))\nplt.plot(prediction, label='Predicted_inbuilt', color='blue')\nplt.plot(z_val[0], label='Predicted_', color='black',linestyle='--')\n# plt.plot(y_val_arr[0], label='Actual', color='red')\n\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.title('Plot of Predicted and Actual Values (Validation Data)')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z_val,y_val_arr,w,b,errors_l1=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=400,reg_penalty=\"L1\",penalty_factor=0.1)\nz_val,y_val_arr,w,b,errors_l2=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=400,reg_penalty=\"L2\",penalty_factor=0.1)\nz_val,y_val_arr,w,b,errors_n=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(errors_l1, label='L1 Penalty', color='blue')\nplt.plot(errors_l2, label='L2 Penalty', color='green')\nplt.plot(errors_n, label='No Penalty', color='red')\n\nplt.xlabel('Iterations/epochs')\nplt.title('Epochs: 400, Learning Rate: 0.01, L1,L2 Penalty Factor: 0.4')\nplt.ylabel('RMSE')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_reg_l1 = []\nerror_reg_l2 = []\npenalty_list=[]\nfor i in range(2, 99):\n    penalty = (i)/100\n    z_val,y_val_arr,w,b,errors_l1=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.5,epochs=100,reg_penalty=\"L1\",penalty_factor=penalty)\n    z_val,y_val_arr,w,b,errors_l2=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.5,epochs=100,reg_penalty=\"L2\",penalty_factor=penalty)\n    error_reg_l1.append(errors_l1[-1])\n    error_reg_l2.append(errors_l2[-1])\n    penalty_list.append(penalty)\nplt.plot(penalty_list,error_reg_l1,label='L1 Penalty', color='blue')\nplt.plot(penalty_list,error_reg_l2,label='L2 Penalty', color='green')\nplt.title('Penalty Factor vs RMSE')\nplt.xlabel('Penalty Factor')\nplt.ylabel('RMSE')\n\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.2, random_state=5)\nz_val,y_val_arr,w,b,errors_80=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.4,epochs=100)\nX_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.3, random_state=5)\nz_val,y_val_arr,w,b,errors_70=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.4,epochs=100)\nX_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.4, random_state=5)\nz_val,y_val_arr,w,b,errors_60=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.4,epochs=100)\nplt.plot(errors_80, label='20-80', color='blue')\nplt.plot(errors_70, label='30-70', color='green')\nplt.plot(errors_60, label='40-60', color='red')\nplt.ylabel('RMSE')\nplt.xlabel('No of iterations/epochs')\nplt.title('Epochs vs RMSE')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### good split could be 40-60\n##### epochs could be 200\n##### L1 penalty factor doesnt affect","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.4, random_state=5)\nz_val,y_val_arr,w,b,errors_l1=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.1,epochs=100,reg_penalty=\"L1\",penalty_factor=0.1)\nprint(w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_split = []\nerror_split_l1=[]\nerror_split_l2=[]\nsplit_list=[]\nfor i in range(1, 80):\n    split= (i)/100\n    X_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=split, random_state=5)\n    z_val,y_val_arr,w,b,errors_norm=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=100)\n    z_val,y_val_arr,w,b,errors_l1=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=100,reg_penalty=\"L1\",penalty_factor=0.1)\n    z_val,y_val_arr,w,b,errors_l2=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=100,reg_penalty=\"L2\",penalty_factor=0.1)\n    error_split.append(errors_norm[-1])\n    error_split_l1.append(errors_l1[-1])\n    error_split_l2.append(errors_l2[-1])\n    split_list.append(split)\nplt.plot(split_list,error_split,label='No Penalty', color='blue')\nplt.plot(split_list,error_split_l1,label='L1 penallty',color='red')\nplt.plot(split_list,error_split_l2,label='L2 penallty',color='green')\nplt.xlabel('Test Split')\nplt.ylabel('RMSE')\nplt.title('Test Split')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyPCA:\n    \n    def __init__(self, n_components):\n        self.n_components = n_components   \n        \n    def fit(self, X):\n        # Standardize data \n        X = X.copy()\n        self.mean = np.mean(X, axis = 0)\n        self.scale = np.std(X, axis = 0)\n        X_std = (X - self.mean) / self.scale\n        \n        # Eigendecomposition of covariance matrix       \n        cov_mat = np.cov(X_std.T)\n        eig_vals, eig_vecs = np.linalg.eig(cov_mat) \n        \n        # Adjusting the eigenvectors that are largest in absolute value to be positive    \n        max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n        signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n        eig_vecs = eig_vecs*signs[np.newaxis,:]\n        eig_vecs = eig_vecs.T\n       \n        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n        eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n        eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n        \n        self.components = eig_vecs_sorted[:self.n_components,:]\n        \n        # Explained variance ratio\n        self.explained_variance_ratio = [i/np.sum(eig_vals) for i in eig_vals_sorted[:self.n_components]]\n        \n        self.cum_explained_variance = np.cumsum(self.explained_variance_ratio)\n\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X_std = (X - self.mean) / self.scale\n        X_proj = X_std.dot(self.components.T)\n        \n        return X_proj\n    \n\n# ---------------------------------------------------------\nmy_pca = MyPCA(n_components = 5).fit(X)\n\nprint('Components:\\n', my_pca.components)\nprint('Explained variance ratio from scratch:\\n', my_pca.explained_variance_ratio)\nprint('Cumulative explained variance from scratch:\\n', my_pca.cum_explained_variance)\n\nX_proj = my_pca.transform(X)\nprint('Transformed data shape from scratch:', X_proj.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split_custom(X_proj, y, test_size=0.4, random_state=5)\nz_val,y_val_arr,w,b,errors_norm=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=200)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_pca = MyPCA(n_components = 5).fit(X)\nX_proj = my_pca.transform(X)\nX_train, X_val, y_train, y_val = train_test_split_custom(X_proj, y, test_size=0.4, random_state=5)\nz_val,y_val_arr,w,b,errors_norm=linear_regresssion_model(X_train, y_train, X_val, y_val,learning_rate=0.2,epochs=200)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(errors_norm)\nplt.xlabel('Iterations/epochs')\nplt.ylabel('RMSE')\n\nplt.title('epochs :- 100  learning_rate 0.01' )\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### lets take upto 5 features from pca and do the polynomial analysis for it ","metadata":{}},{"cell_type":"code","source":"errors_list = []\ncolors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'cyan']\n\nfor i in range(1, 10):\n    my_pca = MyPCA(n_components=i).fit(X)\n    X_proj = my_pca.transform(X)\n    X_train, X_val, y_train, y_val = train_test_split_custom(X_proj, y, test_size=0.4, random_state=5)\n    z_val, y_val_arr, w, b, errors_norm = linear_regresssion_model(X_train, y_train, X_val, y_val, learning_rate=0.2, epochs=200)\n    plt.plot(errors_norm, label=f'n_components={i}', color=colors[i-1])\n\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SVR:\n    def __init__(self, epsilon=0.5):\n        self.epsilon = epsilon\n        self.W = None\n        self.intercept_ = None\n\n    def _compute_loss(self, X, y):\n        y_pred = np.dot(X, self.W) + self.intercept_\n        error = np.maximum(0, np.abs(y_pred - y) - self.epsilon)\n        loss = np.linalg.norm(self.W) / 2 + np.mean(error)\n        return loss\n\n    def fit(self, X, y, epochs=100, learning_rate=0.01):\n        feature_len = X.shape[-1] if len(X.shape) > 1 else 1\n        \n        self.W = np.random.randn(feature_len)\n        self.intercept_ = np.random.randn(1)\n\n        for epoch in range(epochs):\n            loss = self._compute_loss(X, y)\n            # print(\"{}/{}: loss: {}\".format(epoch + 1, epochs, loss))\n\n            # Compute gradients\n            y_pred = np.dot(X, self.W) + self.intercept_\n            error = y_pred - y\n            \n            grad_W = np.dot(X.T, np.where(np.abs(error) > self.epsilon, np.sign(error), 0))\n            grad_b = np.sum(np.where(np.abs(error) > self.epsilon, np.sign(error), 0))\n            # Update weights\n            self.W = self.W - learning_rate * grad_W / len(y)\n            self.intercept_ -= learning_rate * grad_b / len(y)\n        \n        return self\n\n    def predict(self, X):\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        y_pred = np.dot(X, self.W) + self.intercept_\n        return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split_custom_with_array(X, y, test_size=0.2, random_state=None):\n    # Set seed for reproducibility\n    if random_state is not None:\n        np.random.seed(random_state)\n    \n    # Shuffle indices\n    num_samples = len(X)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    \n    # Determine split index\n    split_index = int((1 - test_size) * num_samples)\n    \n    # Split data\n    X_train, X_test = X.iloc[indices[:split_index]].values, X.iloc[indices[split_index:]].values\n    y_train, y_test = y.iloc[indices[:split_index]].values, y.iloc[indices[split_index:]].values\n    \n    return X_train, X_test, y_train, y_test\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_rmse(y_test,y_pred):\n    rmse = 0\n    for i in range(len(y_test)):\n        rmse += (y_pred[i] - y_test[i])**2\n    rmse = (rmse / len(y_test))**0.5\n    return rmse ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split_custom_with_array(X, y, test_size=0.2, random_state=24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=SVR(epsilon=0.5)\nmodel.fit(X_train,y_train,epochs=100,learning_rate=0.01)\ny_pred=model.predict(X_test)\nprint(compute_rmse(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_epsilon = []\nepsilon_values = []\nd_epsilon = 0.01\nwhile d_epsilon <= 0.8:\n    model = SVR(epsilon=d_epsilon)\n    model.fit(X_train, y_train, epochs=100, learning_rate=0.01)\n    y_pred = model.predict(X_test)\n    error = compute_rmse(y_test, y_pred)\n    error_epsilon.append(error)\n    epsilon_values.append(d_epsilon)\n    d_epsilon = round(d_epsilon + 0.01, 2)  # Increment by 0.01 and round to 2 decimal places\n\n# Plotting\nplt.plot(epsilon_values, error_epsilon, marker='o', linestyle='-')\nplt.title('RMSE vs. Epsilon')\nplt.xlabel('Epsilon Value')\nplt.ylabel('RMSE')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_lr = []\nlearning_rates = []\nd_lr = 0.01\nwhile d_lr <= 0.7:\n    model = SVR(epsilon=0.5)\n    model.fit(X_train, y_train, epochs=100, learning_rate=d_lr)\n    y_pred = model.predict(X_test)\n    error = compute_rmse(y_test, y_pred)\n    error_lr.append(error)\n    learning_rates.append(d_lr)\n    d_lr = round(d_lr + 0.01, 2)  \n\nplt.plot(learning_rates, error_lr, marker='o', linestyle='-')\nplt.title('RMSE vs. Learning Rate')\nplt.xlabel('Learning Rate')\nplt.ylabel('RMSE')\nplt.grid(True)\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_epoch = []\nepochs_values = []\nd_epoch = 50\nwhile d_epoch <= 500:\n    model = SVR(epsilon=0.5)\n    model.fit(X_train, y_train, epochs=d_epoch, learning_rate=0.01)\n    y_pred = model.predict(X_test)\n    error = compute_rmse(y_test, y_pred)\n    error_epoch.append(error)\n    epochs_values.append(d_epoch)\n    d_epoch += 50  # Increment by 50 for the next iteration\n\n# Plotting\nplt.plot(epochs_values, error_epoch, marker='o', linestyle='-')\nplt.title('RMSE vs. Number of Epochs')\nplt.xlabel('Number of Epochs')\nplt.ylabel('RMSE')\nplt.grid(True)\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### with pca ","metadata":{}},{"cell_type":"code","source":"rmse_list = []\n\nfor n_features in range(1, 11):\n    my_pca = MyPCA(n_components=n_features).fit(X)\n    X_proj = my_pca.transform(X)\n    X_train, X_val, y_train, y_val = train_test_split_custom_with_array(X_proj, y, test_size=0.4, random_state=5)\n    model = SVR(epsilon=0.5)\n    model.fit(X_train, y_train, epochs=250, learning_rate=0.20)\n    y_pred = model.predict(X_val)\n    rmse = compute_rmse(y_val, y_pred)\n    rmse_list.append(rmse)\n\nprint(rmse_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = list(range(1, 11))\n\n# Plot RMSE values\nplt.plot(n_features, rmse_list, marker='o', linestyle='-')\nplt.xlabel('Number of Features')\nplt.ylabel('RMSE')\nplt.title('RMSE vs Number of Features')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Node():\n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n        ''' constructor ''' \n        \n        # for decision node\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.var_red = var_red\n        \n        # for leaf node\n        self.value = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecisionTreeRegressor():\n    def __init__(self, min_samples_split=2, max_depth=2):\n        ''' constructor '''\n        \n        # initialize the root of the tree \n        self.root = None\n        \n        # stopping conditions\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.column_names = None  # To store column names\n    \n    def build_tree(self, dataset, curr_depth=0):\n        ''' recursive function to build the tree '''\n        \n        X, Y = dataset[:,:-1], dataset[:,-1]\n        num_samples, num_features = np.shape(X)\n        best_split = {}\n        # split until stopping conditions are met\n        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n            # find the best split\n            best_split = self.get_best_split(dataset, num_samples, num_features)\n            # check if information gain is positive\n            if best_split[\"var_red\"]>0:\n                # recur left\n                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n                # recur right\n                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n                # return decision node\n                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n                            left_subtree, right_subtree, best_split[\"var_red\"])\n        \n        # compute leaf node\n        leaf_value = self.calculate_leaf_value(Y)\n        # return leaf node\n        return Node(value=leaf_value)\n    \n    def get_best_split(self, dataset, num_samples, num_features):\n        ''' function to find the best split '''\n        \n        # dictionary to store the best split\n        best_split = {}\n        max_var_red = -float(\"inf\")\n        # loop over all the features\n        for feature_index in range(num_features):\n            feature_values = dataset[:, feature_index]\n            possible_thresholds = np.unique(feature_values)\n            # loop over all the feature values present in the data\n            for threshold in possible_thresholds:\n                # get current split\n                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n                # check if childs are not null\n                if len(dataset_left)>0 and len(dataset_right)>0:\n                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n                    # compute information gain\n                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n                    # update the best split if needed\n                    if curr_var_red>max_var_red:\n                        best_split[\"feature_index\"] = feature_index\n                        best_split[\"threshold\"] = threshold\n                        best_split[\"dataset_left\"] = dataset_left\n                        best_split[\"dataset_right\"] = dataset_right\n                        best_split[\"var_red\"] = curr_var_red\n                        max_var_red = curr_var_red\n                        \n        # return best split\n        return best_split\n    \n    def split(self, dataset, feature_index, threshold):\n        ''' function to split the data '''\n        \n        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n        return dataset_left, dataset_right\n    \n    def variance_reduction(self, parent, l_child, r_child):\n        ''' function to compute variance reduction '''\n        \n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n        return reduction\n    \n    def calculate_leaf_value(self, Y):\n        ''' function to compute leaf node '''\n        \n        val = np.mean(Y)\n        return val\n                \n    def print_tree(self, columns, tree=None, indent=\" \"):\n        ''' function to print the tree '''\n        \n        if not tree:\n            tree = self.root\n\n        if tree.value is not None:\n            print(tree.value)\n\n        else:\n            print(columns[tree.feature_index], \"<=\", tree.threshold, \"?\", tree.var_red)\n            print(\"%sleft:\" % (indent), end=\"\")\n            self.print_tree(columns, tree.left, indent + indent)\n            print(\"%sright:\" % (indent), end=\"\")\n            self.print_tree(columns, tree.right, indent + indent)\n    \n    def fit(self, X, Y):\n        ''' function to train the tree '''\n        \n        dataset = np.concatenate((X, Y), axis=1)\n        self.root = self.build_tree(dataset)\n        self.column_names = columns\n        \n    def make_prediction(self, x, tree):\n        ''' function to predict new dataset '''\n        \n        if tree.value!=None: return tree.value\n        feature_val = x[tree.feature_index]\n        if feature_val<=tree.threshold:\n            return self.make_prediction(x, tree.left)\n        else:\n            return self.make_prediction(x, tree.right)\n    \n    def predict(self, X):\n        ''' function to predict a single data point '''\n        \n        predictions = [self.make_prediction(x, self.root) for x in X]\n        return predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split_custom_with_array(X, y, test_size=0.2, random_state=24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split_custom_with_array(X, y, test_size=0.2, random_state=24)\nregressor = DecisionTreeRegressor(min_samples_split=3, max_depth=3)\ny_train_reshaped =y_train.reshape(-1,1)\ncolumns=list(X.columns)\n\nregressor.fit(X_train,y_train_reshaped)\nregressor.print_tree(columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=regressor.predict(X_test)\ncompute_rmse(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors_mse=[]\nX_train,X_test,y_train,y_test=train_test_split_custom_with_array(X, y, test_size=0.2, random_state=24)\nfor i in range(2,10):\n    regressor = DecisionTreeRegressor(min_samples_split=i, max_depth=3)\n    y_train_reshaped =y_train.reshape(-1,1)\n    columns=list(X.columns)\n    regressor.fit(X_train,y_train_reshaped)\n    regressor.print_tree(columns)\n    y_pred=regressor.predict(X_test)\n    errors_mse.append(compute_rmse(y_test,y_pred))\n    print(i)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the error_mse\nplt.plot(range(2, 10), errors_mse, marker='o')\nplt.xlabel('min_samples_split')\nplt.ylabel('RMSE')\nplt.title('RMSE vs min_samples_split')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors_mse=[]\nX_train,X_test,y_train,y_test=train_test_split_custom_with_array(X, y, test_size=0.2, random_state=24)\nfor i in range(2,7):\n    regressor = DecisionTreeRegressor(min_samples_split=7, max_depth=i)\n    y_train_reshaped =y_train.reshape(-1,1)\n    columns=list(X.columns)\n    regressor.fit(X_train,y_train_reshaped)\n    regressor.print_tree(columns)\n    y_pred=regressor.predict(X_test)\n    errors_mse.append(compute_rmse(y_test,y_pred))\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the error_mse\nplt.plot(range(2, 7), errors_mse, marker='o')\nplt.xlabel('depth of tree')\nplt.ylabel('RMSE')\nplt.title('RMSE vs depth of tree')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_list = []\n\nfor n_features in range(1, 11):\n    my_pca = MyPCA(n_components=n_features).fit(X)\n    X_proj = my_pca.transform(X)\n    X_train,X_test,y_train,y_test=train_test_split_custom_with_array(X_proj, y, test_size=0.2, random_state=24)\n    regressor = DecisionTreeRegressor(min_samples_split=3, max_depth=3)\n    y_train_reshaped =y_train.reshape(-1,1)\n    columns=list(X.columns)\n    regressor.fit(X_train,y_train_reshaped)\n    regressor.print_tree(columns)\n    y_pred=regressor.predict(X_test)\n    errors_mse.append(compute_rmse(y_test,y_pred))\n    rmse_list.append(rmse)\n\nprint(rmse_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = list(range(1, 11))\n\n# Plot RMSE values\nplt.plot(n_features, rmse_list, marker='o', linestyle='-')\nplt.xlabel('Number of Features')\nplt.ylabel('RMSE')\nplt.title('RMSE vs Number of Features with PCA')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}